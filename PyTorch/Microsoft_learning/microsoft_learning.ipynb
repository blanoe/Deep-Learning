{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11.6%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "38.2%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "66.1%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "92.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68.7%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119.3%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\"}\n",
    "\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label], color=\"white\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 8\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the class\n",
    "\n",
    "We define our neural network by subclassing `nn.Module`, and \n",
    "initialize the neural network layers in `__init__`. Every `nn.Module` subclass implements\n",
    "the operations on input data in the `forward` method.\n",
    "\n",
    "Our neural network are composed of the following:\n",
    "- The input layer with 28x28 or 784 features/pixels.\n",
    "- The first linear module takes the input 784 features and transforms it to a hidden layer with 512 features\n",
    "- The ReLU activation function will be applied in the transformation\n",
    "- The second linear module take 512 features as input from the first hidden layer and transforms it to the next hidden layer with 512 features\n",
    "- The ReLU activation function will be applied in the transformation\n",
    "- The third linear module take 512 features as input from the second hidden layer and transforms it to the output layer with 10, which is the number of classes\n",
    "- The ReLU activation function will be applied in the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([1])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X) \n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ReLU\n",
    "\n",
    "Non-linear activations are what create the complex mappings between the model's inputs and outputs.\n",
    "They are applied after linear transformations to introduce *nonlinearity*, helping neural networks\n",
    "learn a wide variety of phenomena. In this model, we use `nn.ReLU` between our linear layers, but there's other activations to introduce non-linearity in your model.\n",
    "\n",
    "The ReLU activation function takes the output from the linear layer calculation and replaces the negative values with zeros.\n",
    "\n",
    "Linear output: ${ x = {weight * input + bias}} $.  \n",
    "ReLU:  $f(x)= \n",
    "\\begin{cases}\n",
    "    0, & \\text{if } x < 0\\\\\n",
    "    x, & \\text{if } x\\geq 0\\\\\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.5587,  0.2821,  0.2062,  0.4672,  0.1026, -0.0795,  0.0967, -0.1367,\n",
      "          0.0751, -0.0281,  0.0033,  0.2924,  1.1141, -0.3699, -0.2461, -0.1337,\n",
      "          0.2733, -0.1139,  0.2933, -0.1426],\n",
      "        [-0.3523,  0.4592,  0.2646,  0.8333, -0.0327, -0.3190,  0.3439, -0.0804,\n",
      "          0.0593, -0.1222, -0.0314, -0.0331,  0.8987, -0.0354, -0.0559, -0.5613,\n",
      "          0.3857,  0.0693,  0.3697, -0.3229],\n",
      "        [-0.4418,  0.3696, -0.2420,  0.4797, -0.2253, -0.3177,  0.7427, -0.2106,\n",
      "         -0.1796, -0.1690, -0.1007, -0.0863,  1.0877, -0.0299, -0.1053, -0.2527,\n",
      "          0.2952, -0.1433,  0.4038, -0.4256]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.2821, 0.2062, 0.4672, 0.1026, 0.0000, 0.0967, 0.0000, 0.0751,\n",
      "         0.0000, 0.0033, 0.2924, 1.1141, 0.0000, 0.0000, 0.0000, 0.2733, 0.0000,\n",
      "         0.2933, 0.0000],\n",
      "        [0.0000, 0.4592, 0.2646, 0.8333, 0.0000, 0.0000, 0.3439, 0.0000, 0.0593,\n",
      "         0.0000, 0.0000, 0.0000, 0.8987, 0.0000, 0.0000, 0.0000, 0.3857, 0.0693,\n",
      "         0.3697, 0.0000],\n",
      "        [0.0000, 0.3696, 0.0000, 0.4797, 0.0000, 0.0000, 0.7427, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.0877, 0.0000, 0.0000, 0.0000, 0.2952, 0.0000,\n",
      "         0.4038, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential\n",
    "\n",
    "`nn.Sequential` is an ordered \n",
    "container of modules. The data is passed through all the modules in the same order as defined. You can use\n",
    "sequential containers to put together a quick network like `seq_modules`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Softmax\n",
    "\n",
    "The last linear layer of the neural network returns `logits` - raw values in \\[`-infty`, `infty`], which are passed to the\n",
    "`nn.Softmax` module. The Softmax activation function is used to calculate the probability of the output from the neural network.  It is only used on the output layer of a neural network.  The results are scaled to values \\[0, 1\\] representing the model's predicted densities for each class. `dim` parameter indicates the dimension along which the result values must sum to 1.  The node with the highest probability predicts the desired output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0269, -0.0124,  0.0065,  ...,  0.0091, -0.0280, -0.0316],\n",
      "        [-0.0137, -0.0064,  0.0103,  ...,  0.0082, -0.0158, -0.0328]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0158, -0.0258], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0412, -0.0422,  0.0355,  ..., -0.0388,  0.0086, -0.0087],\n",
      "        [ 0.0019, -0.0038,  0.0437,  ..., -0.0271, -0.0239, -0.0311]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0297,  0.0165], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0246,  0.0440, -0.0095,  ..., -0.0070,  0.0324, -0.0295],\n",
      "        [ 0.0149,  0.0388, -0.0178,  ..., -0.0003, -0.0310, -0.0010]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0196,  0.0418], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model structure: \", model, \"\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOMATIC DIFFERENTIATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation with ``torch.autograd``\n",
    "\n",
    "When training neural networks, the most frequently used algorithm is\n",
    "**back propagation**. In this algorithm, parameters (model weights) are\n",
    "adjusted according to the **gradient** of the loss function with respect\n",
    "to the given parameter.  The loss function calulates the difference between the expected output and the actual output that a neural network produces.  The goal is to get the result of the loss function as close to zero as possible.  The algorithm traverse backwards through the network network to adjust the weights and bias to retrain the model. That's why it's called back propagation. This back and forward process of retraining the model over time to reduce the loss to 0 is called the gradient descent.\n",
    "\n",
    "To compute those gradients, PyTorch has a built-in differentiation engine\n",
    "called `torch.autograd`. It supports automatic computation of gradient for any\n",
    "computational graph.\n",
    "\n",
    "Consider the simplest one-layer neural network, with input `x`,\n",
    "parameters `w` and `b`, and some loss function. It can be defined in\n",
    "PyTorch in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x7fc05209aba8>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7fc05209a7f0>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z =',z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing gradients\n",
    "\n",
    "To optimize weights of parameters in the neural network, we need to\n",
    "compute the derivatives of our loss function with respect to parameters,\n",
    "namely, we need $\\frac{\\partial loss}{\\partial w}$ and\n",
    "$\\frac{\\partial loss}{\\partial b}$ under some fixed values of\n",
    "`x` and `y`. To compute those derivatives, we call\n",
    "`loss.backward()`, and then retrieve the values from `w.grad` and\n",
    "`b.grad`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0630, 0.0979, 0.2267],\n",
      "        [0.0630, 0.0979, 0.2267],\n",
      "        [0.0630, 0.0979, 0.2267],\n",
      "        [0.0630, 0.0979, 0.2267],\n",
      "        [0.0630, 0.0979, 0.2267]])\n",
      "tensor([0.0630, 0.0979, 0.2267])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** We can only obtain the `grad` properties for the leaf nodes of the computational graph, which have `requires_grad` property set to ``True``. For all other nodes in our graph, gradients will not be available. In addition, we can only perform gradient calculations using ``backward`` once on a given graph, for performance reasons. If we need to do several ``backward`` calls on the same graph, we need to pass ``retain_graph=True`` to the ``backward`` call.\n",
    "\n",
    "## Disabling gradient tracking\n",
    "\n",
    "By default, all tensors with `requires_grad=True` are tracking their\n",
    "computational history and support gradient computation. However, there\n",
    "are some cases when we do not need to do that, for example, when we have\n",
    "trained the model and just want to apply it to some input data, i.e. we\n",
    "only want to do *forward* computations through the network. We can stop\n",
    "tracking computations by surrounding our computation code with\n",
    "`torch.no_grad()` block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyperparameters \n",
    "\n",
    "Hyperparameters are adjustable parameters that let you control the model optimization process. \n",
    "Different hyperparameter values can impact model training and the level of accuracy.\n",
    "\n",
    "We define the following hyperparameters for training:\n",
    " - **Number of Epochs** - the number times the entire training dataset is pass through the network. \n",
    " - **Batch Size** - the number of data samples seen by the model in each epoch. Iterates are the number of batches needs to compete an epoch.\n",
    " - **Learning Rate** - the size of steps the model match as it searchs for best weights that will produce a higher model accuracy. Smaller values means the model will take a longer time to find the best weights, while larger values may result in the model step over and misses the best weights which yields unpredictable behavior during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add an optimization loop\n",
    "\n",
    "Once we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each \n",
    "iteration of the optimization loop is called an **epoch**. \n",
    "\n",
    "Each epoch consists of two main parts:\n",
    " - **The Train Loop** - iterate over the training dataset and try to converge to optimal parameters.\n",
    " - **The Validation/Test Loop** - iterate over the test dataset to check if model performance is improving.\n",
    "\n",
    "Let's briefly familiarize ourselves with some of the concepts used in the training loop. Jump ahead to \n",
    "see the `full-impl-label` of the optimization loop.\n",
    "\n",
    "### Add a loss function\n",
    "\n",
    "When presented with some training data, our untrained network is likely not to give the correct \n",
    "answer. **Loss function** measures the degree of dissimilarity of obtained result to the target value, \n",
    "and it is the loss function that we want to minimize during training. To calculate the loss we make a \n",
    "prediction using the inputs of our given data sample and compare it against the true data label value.\n",
    "\n",
    "Common loss functions include:\n",
    "- `nn.MSELoss` (Mean Square Error) used for regression tasks\n",
    "- `nn.NLLLoss` (Negative Log Likelihood) used for classification\n",
    "- `nn.CrossEntropyLoss` combines `nn.LogSoftmax` and `nn.NLLLoss`\n",
    "\n",
    "We pass our model's output logits to `nn.CrossEntropyLoss`, which will normalize the logits and compute the prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization pass\n",
    "\n",
    "Optimization is the process of adjusting model parameters to reduce model error in each training step. **Optimization algorithms** define how this process is performed (in this example we use Stochastic Gradient Descent).\n",
    "All optimization logic is encapsulated in  the ``optimizer`` object. Here, we use the SGD optimizer; additionally, there are many different optimizers\n",
    "available in PyTorch such as `ADAM' and 'RMSProp`, that work better for different kinds of models and data.\n",
    "\n",
    "We initialize the optimizer by registering the model's parameters that need to be trained, and passing in the learning rate hyperparameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the training loop, optimization happens in three steps:\n",
    " * Call `optimizer.zero_grad()` to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    " * Back-propagate the prediction loss with a call to `loss.backwards()`. PyTorch deposits the gradients of the loss w.r.t. each parameter. \n",
    " * Once we have our gradients, we call ``optimizer.step()`` to adjust the parameters by the gradients collected in the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full implementation\n",
    "\n",
    "We define `train_loop` that loops over our optimization code, and `test_loop` that \n",
    "evaluates the model's performance against our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):        \n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.303636  [    0/60000]\n",
      "loss: 2.301039  [ 6400/60000]\n",
      "loss: 2.290618  [12800/60000]\n",
      "loss: 2.289658  [19200/60000]\n",
      "loss: 2.280269  [25600/60000]\n",
      "loss: 2.254221  [32000/60000]\n",
      "loss: 2.264694  [38400/60000]\n",
      "loss: 2.245009  [44800/60000]\n",
      "loss: 2.223163  [51200/60000]\n",
      "loss: 2.228556  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.5%, Avg loss: 0.035119 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.216347  [    0/60000]\n",
      "loss: 2.240642  [ 6400/60000]\n",
      "loss: 2.221061  [12800/60000]\n",
      "loss: 2.240096  [19200/60000]\n",
      "loss: 2.226688  [25600/60000]\n",
      "loss: 2.160177  [32000/60000]\n",
      "loss: 2.195775  [38400/60000]\n",
      "loss: 2.155865  [44800/60000]\n",
      "loss: 2.112072  [51200/60000]\n",
      "loss: 2.135969  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.8%, Avg loss: 0.033861 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.111733  [    0/60000]\n",
      "loss: 2.162685  [ 6400/60000]\n",
      "loss: 2.132719  [12800/60000]\n",
      "loss: 2.177107  [19200/60000]\n",
      "loss: 2.155516  [25600/60000]\n",
      "loss: 2.040992  [32000/60000]\n",
      "loss: 2.100947  [38400/60000]\n",
      "loss: 2.039141  [44800/60000]\n",
      "loss: 1.969372  [51200/60000]\n",
      "loss: 2.017807  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 0.032285 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.977097  [    0/60000]\n",
      "loss: 2.062721  [ 6400/60000]\n",
      "loss: 2.022984  [12800/60000]\n",
      "loss: 2.101742  [19200/60000]\n",
      "loss: 2.071135  [25600/60000]\n",
      "loss: 1.910930  [32000/60000]\n",
      "loss: 1.991836  [38400/60000]\n",
      "loss: 1.916124  [44800/60000]\n",
      "loss: 1.823886  [51200/60000]\n",
      "loss: 1.897879  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 0.030698 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.839406  [    0/60000]\n",
      "loss: 1.960949  [ 6400/60000]\n",
      "loss: 1.915582  [12800/60000]\n",
      "loss: 2.031391  [19200/60000]\n",
      "loss: 1.990336  [25600/60000]\n",
      "loss: 1.795425  [32000/60000]\n",
      "loss: 1.896148  [38400/60000]\n",
      "loss: 1.815921  [44800/60000]\n",
      "loss: 1.712298  [51200/60000]\n",
      "loss: 1.803294  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 42.0%, Avg loss: 0.029391 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.731393  [    0/60000]\n",
      "loss: 1.874515  [ 6400/60000]\n",
      "loss: 1.827193  [12800/60000]\n",
      "loss: 1.974709  [19200/60000]\n",
      "loss: 1.918580  [25600/60000]\n",
      "loss: 1.704504  [32000/60000]\n",
      "loss: 1.822236  [38400/60000]\n",
      "loss: 1.742012  [44800/60000]\n",
      "loss: 1.633963  [51200/60000]\n",
      "loss: 1.733480  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.3%, Avg loss: 0.028348 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.650799  [    0/60000]\n",
      "loss: 1.806266  [ 6400/60000]\n",
      "loss: 1.757209  [12800/60000]\n",
      "loss: 1.927507  [19200/60000]\n",
      "loss: 1.859689  [25600/60000]\n",
      "loss: 1.634815  [32000/60000]\n",
      "loss: 1.764263  [38400/60000]\n",
      "loss: 1.685292  [44800/60000]\n",
      "loss: 1.576026  [51200/60000]\n",
      "loss: 1.682487  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.027514 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.589386  [    0/60000]\n",
      "loss: 1.753518  [ 6400/60000]\n",
      "loss: 1.700298  [12800/60000]\n",
      "loss: 1.888657  [19200/60000]\n",
      "loss: 1.813719  [25600/60000]\n",
      "loss: 1.580777  [32000/60000]\n",
      "loss: 1.720742  [38400/60000]\n",
      "loss: 1.640795  [44800/60000]\n",
      "loss: 1.531386  [51200/60000]\n",
      "loss: 1.648666  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 44.2%, Avg loss: 0.026867 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.541864  [    0/60000]\n",
      "loss: 1.712338  [ 6400/60000]\n",
      "loss: 1.654851  [12800/60000]\n",
      "loss: 1.858325  [19200/60000]\n",
      "loss: 1.777319  [25600/60000]\n",
      "loss: 1.539424  [32000/60000]\n",
      "loss: 1.687427  [38400/60000]\n",
      "loss: 1.606340  [44800/60000]\n",
      "loss: 1.496303  [51200/60000]\n",
      "loss: 1.624402  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 44.6%, Avg loss: 0.026354 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.502878  [    0/60000]\n",
      "loss: 1.678787  [ 6400/60000]\n",
      "loss: 1.617735  [12800/60000]\n",
      "loss: 1.831965  [19200/60000]\n",
      "loss: 1.747287  [25600/60000]\n",
      "loss: 1.506401  [32000/60000]\n",
      "loss: 1.660297  [38400/60000]\n",
      "loss: 1.578206  [44800/60000]\n",
      "loss: 1.467184  [51200/60000]\n",
      "loss: 1.606173  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.1%, Avg loss: 0.025919 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Models\n",
    "-------------\n",
    "\n",
    "When you are satisfied with the model's performance, you can use `torch.save` to save it. PyTorch models store the learned parameters in an internal state dictionary, called `state_dict`. These can be persisted wit the `torch.save` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"data/model.pth\")\n",
    "\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and run model predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import onnxruntime\n",
    "from torch import nn\n",
    "import torch.onnx as onnx\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load('data/model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.zeros((1,28,28))\n",
    "onnx_model = 'data/model.onnx'\n",
    "onnx.export(model, input_image, onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "x, y = test_data[0][0], test_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Sandal\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "session = onnxruntime.InferenceSession(onnx_model, None)\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "result = session.run([output_name], {input_name: x.numpy()})\n",
    "predicted, actual = classes[result[0][0].argmax(0)], classes[y]\n",
    "print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a893b163cb2ff74ce7ce398d3a7dd97f55c79978673b1ab6076b685bfe536192"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('pytorch_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
